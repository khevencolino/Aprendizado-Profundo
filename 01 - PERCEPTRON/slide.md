---
title: Perceptron Multicamadas para AproximaÃ§Ã£o de FunÃ§Ãµes
sub_title: Teorema da AproximaÃ§Ã£o Universal
author: Kheven
date: 2025
options:
  end_slide_shorthand: true
theme:
  name: gruvbox-dark
---

# Perceptron Multicamadas para AproximaÃ§Ã£o de FunÃ§Ãµes

## Teorema da AproximaÃ§Ã£o Universal na PrÃ¡tica

---

## Objetivo

ğŸ¯ **Demonstrar a capacidade de aproximaÃ§Ã£o universal das redes neurais**

- **FunÃ§Ã£o A**: f(x) = sin(2x) + cos(3x) (trigonomÃ©trica)
- **FunÃ§Ã£o B**: f(x) = 10xâµ + 5xâ´ + 2xÂ³ - 0.5xÂ² + 3x + 2 (polinomial)
- **Arquitetura**: Perceptron Multicamadas (MLP)
- **Framework**: Keras com arquiteturas otimizadas

---

## Teorema da AproximaÃ§Ã£o Universal

ğŸ§  **Fundamento teÃ³rico:**

### O que diz o teorema:

> _"Uma rede neural feedforward com uma Ãºnica camada oculta pode aproximar qualquer funÃ§Ã£o contÃ­nua com precisÃ£o arbitrÃ¡ria, dado neurÃ´nios suficientes."_

### Na prÃ¡tica:

- **FunÃ§Ãµes complexas** podem ser aprendidas
- **NÃ£o-linearidades** sÃ£o capturadas eficientemente
- **Mapeamento universal** entre entrada e saÃ­da

### Desafios:

- Encontrar a **arquitetura adequada**
- **Evitar overfitting** com regularizaÃ§Ã£o
- **Otimizar treinamento** com callbacks

---

## FunÃ§Ãµes Target

ğŸ“ **Duas funÃ§Ãµes com caracterÃ­sticas distintas:**

### FunÃ§Ã£o A - TrigonomÃ©trica:

```python
def function_a(x):
    return np.sin(2*x) + np.cos(3*x)
```

- **CaracterÃ­sticas**: PeriÃ³dica, limitada [-2, 2]
- **Complexidade**: OscilaÃ§Ãµes mÃºltiplas
- **Desafio**: Capturar padrÃµes periÃ³dicos

### FunÃ§Ã£o B - Polinomial de Alta Ordem:

```python
def function_b(x):
    return 10*x**5 + 5*x**4 + 2*x**3 - 0.5*x**2 + 3*x + 2
```

- **CaracterÃ­sticas**: Crescimento explosivo, valores grandes
- **Complexidade**: NÃ£o-linearidade extrema
- **Desafio**: Lidar com diferentes escalas

---

## GeraÃ§Ã£o dos Dados

ğŸ“Š **EstratÃ©gia de amostragem:**

```python
def generate_dataset(func, x_min=0, x_max=5, n_samples=4000):
    x = np.random.uniform(x_min, x_max, n_samples)
    y = func(x)
    return x.reshape(-1, 1), y

# DivisÃ£o dos dados
# 60% Treinamento | 20% ValidaÃ§Ã£o | 20% Teste
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)
```

### CaracterÃ­sticas:

- **4.000 amostras** por funÃ§Ã£o
- **Amostragem uniforme** no intervalo [0, 5]
- **DivisÃ£o estratificada** para validaÃ§Ã£o robusta

---

## PrÃ©-processamento

ğŸ”§ **Tratamento diferenciado por funÃ§Ã£o:**

### FunÃ§Ã£o A (TrigonomÃ©trica):

- **Sem normalizaÃ§Ã£o**: Valores jÃ¡ em escala adequada [-2, 2]
- **Dados diretos** para treinamento

### FunÃ§Ã£o B (Polinomial):

```python
# NormalizaÃ§Ã£o obrigatÃ³ria devido aos valores grandes
scaler_x_b = StandardScaler()
scaler_y_b = StandardScaler()

x_train_b_scaled = scaler_x_b.fit_transform(x_train_b)
y_train_b_scaled = scaler_y_b.fit_transform(y_train_b.reshape(-1, 1))
```

**RazÃ£o**: FunÃ§Ã£o B produz valores de 2 atÃ© 100.000+, causando instabilidade numÃ©rica sem normalizaÃ§Ã£o.

---

## Arquitetura dos Modelos

ğŸ—ï¸ **MLPs otimizados para cada funÃ§Ã£o:**

### Modelo A (FunÃ§Ã£o TrigonomÃ©trica):

```python
model_a = Sequential([
    Dense(64, activation='tanh', input_dim=1),
    Dense(32, activation='tanh'),
    Dense(16, activation='tanh'),
    Dense(1, activation='linear')
])
```

### Modelo B (FunÃ§Ã£o Polinomial):

```python
model_b = Sequential([
    Dense(128, activation='tanh', input_dim=1),
    Dense(64, activation='tanh'),
    Dense(32, activation='tanh'),
    Dense(16, activation='tanh'),
    Dense(1, activation='linear')
])
```

---

## Detalhes da Arquitetura

ğŸ“ **EspecificaÃ§Ãµes tÃ©cnicas:**

| Aspecto             | Modelo A | Modelo B     |
| ------------------- | -------- | ------------ |
| **Camadas Ocultas** | 3        | 4            |
| **NeurÃ´nios**       | 64â†’32â†’16 | 128â†’64â†’32â†’16 |
| **AtivaÃ§Ã£o**        | tanh     | tanh         |
| **ParÃ¢metros**      | ~3K      | ~11K         |
| **Complexidade**    | Menor    | Maior        |

### Por que tanh?

- **SimÃ©trica**: Melhor para aproximaÃ§Ã£o de funÃ§Ãµes
- **Gradientes**: Menos problemas de saturaÃ§Ã£o que sigmoid
- **Range**: [-1, 1] adequado para funÃ§Ãµes normalizadas

### Por que mais neurÃ´nios no Modelo B?

- **Maior complexidade** da funÃ§Ã£o polinomial
- **Necessidade de capturar** crescimento exponencial

---

## ConfiguraÃ§Ã£o do Treinamento

âš™ï¸ **OtimizaÃ§Ã£o:**

```python
# CompilaÃ§Ã£o
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

# Callbacks para controle automÃ¡tico
callbacks = [
    EarlyStopping(monitor='val_loss', patience=20),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),
    StopTrainingAtAccuracy(target=0.99)  # Custom callback
]

# Treinamento
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=200, batch_size=32,
    callbacks=callbacks
)
```

---

## Resultados - FunÃ§Ã£o A (TrigonomÃ©trica)

ğŸ† **MÃ©tricas de performance:**

| MÃ©trica    | Valor    | InterpretaÃ§Ã£o                 |
| ---------- | -------- | ----------------------------- |
| **MSE**    | 0.000021 | Erro quadrÃ¡tico muito baixo   |
| **RMSE**   | 0.0046   | Erro mÃ©dio de ~0.005          |
| **RÂ²**     | 0.9979   | 99.79% da variÃ¢ncia explicada |
| **Ã‰pocas** | 80-120   | ConvergÃªncia rÃ¡pida           |

### ObservaÃ§Ãµes:

- **Excelente aproximaÃ§Ã£o** da funÃ§Ã£o periÃ³dica
- **Captura perfeita** das oscilaÃ§Ãµes
- **GeneralizaÃ§Ã£o robusta** no conjunto de teste

---

## Resultados - FunÃ§Ã£o B (Polinomial)

ğŸ† **MÃ©tricas de performance:**

| MÃ©trica    | Valor    | InterpretaÃ§Ã£o                  |
| ---------- | -------- | ------------------------------ |
| **MSE**    | 0.000002 | Erro quase imperceptÃ­vel       |
| **RMSE**   | 0.0014   | PrecisÃ£o extrema               |
| **RÂ²**     | 0.99998  | 99.998% da variÃ¢ncia explicada |
| **Ã‰pocas** | 60-100   | ConvergÃªncia eficiente         |

### ObservaÃ§Ãµes:

- **AproximaÃ§Ã£o quase perfeita** da funÃ§Ã£o polinomial
- **NormalizaÃ§Ã£o foi crucial** para estabilidade
- **Arquitetura mais profunda** foi necessÃ¡ria

---

## VisualizaÃ§Ã£o dos Resultados

![](output.png)

- _FunÃ§Ã£o A: Real vs Predito_
- _FunÃ§Ã£o B: Real vs Predito_
- _Erro Absoluto - FunÃ§Ã£o A_
- _Erro Absoluto - FunÃ§Ã£o B_
- SobreposiÃ§Ã£o **quase perfeita** entre curvas real e predita
- Erros **concentrados em regiÃµes** de maior curvatura
- **GeneralizaÃ§Ã£o excelente** em todo o domÃ­nio

---

## Curvas de Aprendizado

![](output2.png)

- _Loss vs Ã‰pocas (FunÃ§Ã£o A)_
- _MAE vs Ã‰pocas (FunÃ§Ã£o A)_
- _Loss vs Ã‰pocas (FunÃ§Ã£o B)_
- _MAE vs Ã‰pocas (FunÃ§Ã£o B)_
- **ConvergÃªncia estÃ¡vel** sem oscilaÃ§Ãµes
- **Sem overfitting** significativo
- **Early stopping** otimizou nÃºmero de Ã©pocas
- **Learning rate reduction** melhorou convergÃªncia final

---

## ComparaÃ§Ã£o dos Modelos

ğŸ“Š **AnÃ¡lise comparativa:**

| Aspecto           | FunÃ§Ã£o A       | FunÃ§Ã£o B    |
| ----------------- | -------------- | ----------- |
| **Tipo**          | TrigonomÃ©trica | Polinomial  |
| **Complexidade**  | MÃ©dia          | Alta        |
| **NormalizaÃ§Ã£o**  | NÃ£o necessÃ¡ria | ObrigatÃ³ria |
| **Arquitetura**   | 3 camadas      | 4 camadas   |
| **ParÃ¢metros**    | 3.185          | 11.537      |
| **RÂ² final**      | 99.79%         | 99.998%     |
| **Ã‰pocas mÃ©dias** | 100            | 80          |

### Insights:

- **FunÃ§Ã£o polinomial** requer mais parÃ¢metros mas converge mais rÃ¡pido
- **NormalizaÃ§Ã£o** Ã© crÃ­tica para funÃ§Ãµes com grande variaÃ§Ã£o de escala
- **Ambas as arquiteturas** demonstram capacidade de aproximaÃ§Ã£o universal

---

## Principais Aprendizados

ğŸ“ **Insights tÃ©cnicos:**

### Sobre AproximaÃ§Ã£o Universal:

- **MLPs simples** podem aproximar funÃ§Ãµes complexas
- **Profundidade** Ã© mais importante que largura para certas funÃ§Ãµes
- **AtivaÃ§Ã£o tanh** Ã© superior a ReLU para aproximaÃ§Ã£o

### Sobre PrÃ©-processamento:

- **NormalizaÃ§Ã£o** Ã© crÃ­tica para estabilidade numÃ©rica
- **Escala dos dados** afeta dramaticamente a convergÃªncia
- **Diferentes funÃ§Ãµes** requerem tratamentos especÃ­ficos

### Sobre Treinamento:

- **Callbacks automÃ¡ticos** otimizam o processo
- **ValidaÃ§Ã£o constante** previne overfitting
- **Arquitetura adequada** Ã© mais importante que forÃ§a bruta

---

## LimitaÃ§Ãµes e Desafios

âš ï¸ **Pontos de atenÃ§Ã£o:**

### LimitaÃ§Ãµes teÃ³ricas:

- **AproximaÃ§Ã£o**, nÃ£o igualdade exata
- **Dependente de dados** de treinamento adequados
- **ExtrapolaÃ§Ã£o limitada** fora do domÃ­nio de treino

### Desafios prÃ¡ticos:

- **Escolha da arquitetura** ainda Ã© arte
- **HiperparÃ¢metros** requerem experimentaÃ§Ã£o
- **FunÃ§Ãµes descontÃ­nuas** sÃ£o mais desafiadoras

### ConsideraÃ§Ãµes computacionais:

- **Trade-off** entre precisÃ£o e complexidade
- **Tempo de treinamento** cresce com complexidade
- **MemÃ³ria** limitada para arquiteturas muito grandes

---

## ConclusÃµes

âœ… **Objetivos alcanÃ§ados:**

- ğŸ¯ **DemonstraÃ§Ã£o prÃ¡tica** do Teorema da AproximaÃ§Ã£o Universal
- ğŸ“ˆ **AproximaÃ§Ã£o excelente** (RÂ² > 99.7%) para ambas as funÃ§Ãµes
- ğŸ—ï¸ **Arquiteturas otimizadas** para diferentes tipos de funÃ§Ã£o
- ğŸ¤– **AutomaÃ§Ã£o inteligente** do processo de treinamento

# Obrigado!

**PrÃ³ximo projeto:** ClassificaÃ§Ã£o BinÃ¡ria com MLP

---
