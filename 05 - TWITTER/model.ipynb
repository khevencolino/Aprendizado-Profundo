{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfba3658",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos em Tweets usando LSTM\n",
    "\n",
    "Este notebook implementa uma Rede Neural Recorrente (RNN) usando LSTM para análise de sentimentos em postagens do Twitter.\n",
    "\n",
    "## Objetivos:\n",
    "- Pré-processar dados de tweets\n",
    "- Implementar uma arquitetura LSTM\n",
    "- Treinar o modelo para classificação de sentimentos (positivo/negativo)\n",
    "- Avaliar o desempenho e apresentar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ff9a0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_tf2: Não foi possível encontrar o módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense, Dropout, Bidirectional\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tokenizers \u001b[38;5;28;01mas\u001b[39;00m tokenizers\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ReduceLROnPlateau\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\keras_hub\\__init__.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers \u001b[38;5;28;01mas\u001b[39;00m layers\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models \u001b[38;5;28;01mas\u001b[39;00m models\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m samplers \u001b[38;5;28;01mas\u001b[39;00m samplers\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tokenizers \u001b[38;5;28;01mas\u001b[39;00m tokenizers\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\keras_hub\\models\\__init__.py:279\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    274\u001b[0m     GemmaTokenizer \u001b[38;5;28;01mas\u001b[39;00m GemmaTokenizer,\n\u001b[0;32m    275\u001b[0m )\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_backbone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    277\u001b[0m     Gemma3Backbone \u001b[38;5;28;01mas\u001b[39;00m Gemma3Backbone,\n\u001b[0;32m    278\u001b[0m )\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_causal_lm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    280\u001b[0m     Gemma3CausalLM \u001b[38;5;28;01mas\u001b[39;00m Gemma3CausalLM,\n\u001b[0;32m    281\u001b[0m )\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_causal_lm_preprocessor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    283\u001b[0m     Gemma3CausalLMPreprocessor \u001b[38;5;28;01mas\u001b[39;00m Gemma3CausalLMPreprocessor,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    286\u001b[0m     Gemma3Tokenizer \u001b[38;5;28;01mas\u001b[39;00m Gemma3Tokenizer,\n\u001b[0;32m    287\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\keras_hub\\src\\models\\gemma3\\gemma3_causal_lm.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcausal_lm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalLM\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_backbone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Gemma3Backbone\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgemma3_causal_lm_preprocessor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     Gemma3CausalLMPreprocessor,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m any_equal\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\keras_hub\\src\\models\\gemma3\\gemma3_causal_lm_preprocessor.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_hub_export\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_segment_packer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     MultiSegmentPacker,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\tensorflow\\__init__.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF2_BEHAVIOR\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n",
      "File \u001b[1;32mc:\\Users\\Kheven\\.conda\\envs\\ml_amd\\lib\\site-packages\\tensorflow\\python\\tf2.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools to help with the TensorFlow 2.0 transition.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module is meant for TensorFlow internal implementation, not for users of\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mthe TensorFlow library. For that see tf.compat instead.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21menable\u001b[39m():\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m# Enables v2 behaviors.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: Não foi possível encontrar o módulo especificado."
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para pré-processamento\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from keras_hub import tokenizers as tokenizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91584a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos dados\n",
    "print(\"Carregando dados...\")\n",
    "try:\n",
    "    # Carregando apenas uma amostra do dataset (50k registros)\n",
    "    df = pd.read_csv('DATA/data.csv', encoding='latin-1', nrows=50000)\n",
    "    print(f\"Dataset carregado com sucesso! Shape: {df.shape}\")\n",
    "    \n",
    "    # Verificando as primeiras linhas\n",
    "    print(\"\\nPrimeiras 5 linhas do dataset:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Informações sobre o dataset\n",
    "    print(f\"\\nInformações do dataset:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Verificando valores únicos na coluna de sentimento\n",
    "    print(f\"\\nValores únicos na coluna de sentimento:\")\n",
    "    print(df.iloc[:, 0].value_counts())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "    print(\"Tentando descobrir a estrutura do arquivo...\")\n",
    "    \n",
    "    # Ler apenas as primeiras linhas para entender a estrutura\n",
    "    sample = pd.read_csv('DATA/data.csv', encoding='latin-1', nrows=10)\n",
    "    print(sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para pré-processamento de texto\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Função para limpar e pré-processar os textos dos tweets\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Converter para string e lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remover URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Remover menções (@usuario)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remover hashtags (# mas manter o texto)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remover caracteres especiais e números\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preparar os dados\n",
    "print(\"Preparando dados para análise...\")\n",
    "\n",
    "# Assumindo que a primeira coluna é o sentimento (0=negativo, 4=positivo) \n",
    "# e a última coluna é o texto do tweet\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Converter sentimento: 0 -> 0 (negativo), 4 -> 1 (positivo)\n",
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "# Aplicar pré-processamento\n",
    "print(\"Aplicando pré-processamento no texto...\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Remover textos vazios\n",
    "df = df[df['cleaned_text'].str.len() > 0]\n",
    "\n",
    "print(f\"Dataset após pré-processamento: {df.shape}\")\n",
    "print(f\"Distribuição de sentimentos:\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Mostrar exemplos de textos limpos\n",
    "print(\"\\nExemplos de textos antes e depois do pré-processamento:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['text'].iloc[i]}\")\n",
    "    print(f\"Limpo: {df['cleaned_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização e preparação das sequências\n",
    "print(\"Preparando tokenização...\")\n",
    "\n",
    "# Parâmetros\n",
    "MAX_VOCAB_SIZE = 10000  # Tamanho máximo do vocabulário\n",
    "MAX_SEQUENCE_LENGTH = 100  # Tamanho máximo das sequências\n",
    "EMBEDDING_DIM = 100  # Dimensão dos embeddings\n",
    "\n",
    "# Separar features e target\n",
    "X = df['cleaned_text'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {len(X_train)} exemplos\")\n",
    "print(f\"Conjunto de teste: {len(X_test)} exemplos\")\n",
    "\n",
    "# Tokenização\n",
    "import keras_hub\n",
    "tokenizer = keras_hub.tokenizers.Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokens = tokenizer(X_train.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Converter textos em sequências\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding das sequências\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print(f\"Shape dos dados de treino: {X_train_pad.shape}\")\n",
    "print(f\"Shape dos dados de teste: {X_test_pad.shape}\")\n",
    "\n",
    "# Informações sobre o vocabulário\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 para o token de padding\n",
    "print(f\"Tamanho do vocabulário: {vocab_size}\")\n",
    "print(f\"Tamanho efetivo usado: {min(vocab_size, MAX_VOCAB_SIZE)}\")\n",
    "\n",
    "# Mostrar exemplo de sequência\n",
    "print(f\"\\nExemplo de texto original: {X_train[0]}\")\n",
    "print(f\"Sequência correspondente: {X_train_seq[0][:10]}...\")  # Primeiros 10 tokens\n",
    "print(f\"Sequência com padding: {X_train_pad[0][:15]}...\")  # Primeiros 15 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec76fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do modelo LSTM\n",
    "print(\"Criando modelo LSTM...\")\n",
    "\n",
    "# Parâmetros do modelo\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT_RATE = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "def create_lstm_model():\n",
    "    \"\"\"\n",
    "    Cria um modelo LSTM simples para análise de sentimentos\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Camada de Embedding\n",
    "        Embedding(\n",
    "            input_dim=min(vocab_size, MAX_VOCAB_SIZE),\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        \n",
    "        # Camada LSTM bidirecional\n",
    "        Bidirectional(LSTM(LSTM_UNITS, dropout=DROPOUT_RATE, recurrent_dropout=DROPOUT_RATE)),\n",
    "        \n",
    "        # Camadas Dense\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # Camada de saída\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Criar o modelo\n",
    "model = create_lstm_model()\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Mostrar arquitetura do modelo\n",
    "print(\"Arquitetura do modelo:\")\n",
    "model.summary()\n",
    "\n",
    "# Visualizar parâmetros do modelo\n",
    "print(f\"\\nParâmetros do modelo:\")\n",
    "print(f\"- Tamanho do vocabulário: {min(vocab_size, MAX_VOCAB_SIZE)}\")\n",
    "print(f\"- Dimensão dos embeddings: {EMBEDDING_DIM}\")\n",
    "print(f\"- Tamanho máximo da sequência: {MAX_SEQUENCE_LENGTH}\")\n",
    "print(f\"- Unidades LSTM: {LSTM_UNITS}\")\n",
    "print(f\"- Taxa de dropout: {DROPOUT_RATE}\")\n",
    "print(f\"- Taxa de aprendizado: {LEARNING_RATE}\")\n",
    "\n",
    "# Contar parâmetros treináveis\n",
    "trainable_params = model.count_params()\n",
    "print(f\"- Total de parâmetros treináveis: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "print(\"Iniciando treinamento...\")\n",
    "\n",
    "# Callbacks para melhor treinamento\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Parâmetros de treinamento\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Treinamento concluído!\")\n",
    "\n",
    "# Salvar o modelo\n",
    "model.save('modelo_lstm_sentimentos.keras')\n",
    "print(\"Modelo salvo como 'modelo_lstm_sentimentos.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização das curvas de treinamento\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plota as curvas de loss e accuracy durante o treinamento\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot da Loss\n",
    "    ax1.plot(history.history['loss'], label='Loss de Treino', color='blue')\n",
    "    ax1.plot(history.history['val_loss'], label='Loss de Validação', color='red')\n",
    "    ax1.set_title('Curva de Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Épocas')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot da Accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Accuracy de Treino', color='blue')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Accuracy de Validação', color='red')\n",
    "    ax2.set_title('Curva de Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Épocas')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotar curvas\n",
    "plot_training_history(history)\n",
    "\n",
    "# Estatísticas do treinamento\n",
    "print(\"Estatísticas do treinamento:\")\n",
    "print(f\"- Número de épocas executadas: {len(history.history['loss'])}\")\n",
    "print(f\"- Loss final de treino: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"- Loss final de validação: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"- Accuracy final de treino: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"- Accuracy final de validação: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Identificar melhor época\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"- Melhor época: {best_epoch + 1}\")\n",
    "print(f\"- Melhor loss de validação: {history.history['val_loss'][best_epoch]:.4f}\")\n",
    "print(f\"- Accuracy na melhor época: {history.history['val_accuracy'][best_epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35050599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação do modelo no conjunto de teste\n",
    "print(\"Avaliando modelo no conjunto de teste...\")\n",
    "\n",
    "# Fazer predições\n",
    "y_pred_proba = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calcular métricas\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy no conjunto de teste: {test_accuracy:.4f}\")\n",
    "\n",
    "# Relatório de classificação\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negativo', 'Positivo']))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negativo', 'Positivo'],\n",
    "            yticklabels=['Negativo', 'Positivo'])\n",
    "plt.title('Matriz de Confusão', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n",
    "\n",
    "# Estatísticas detalhadas\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMétricas detalhadas:\")\n",
    "print(f\"- Precisão: {precision:.4f}\")\n",
    "print(f\"- Recall: {recall:.4f}\")\n",
    "print(f\"- F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Análise de distribuição das predições\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_pred_proba, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribuição das Probabilidades Preditas')\n",
    "plt.xlabel('Probabilidade')\n",
    "plt.ylabel('Frequência')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "labels = ['Negativo', 'Positivo']\n",
    "counts = [np.sum(y_pred == 0), np.sum(y_pred == 1)]\n",
    "plt.bar(labels, counts, color=['lightcoral', 'lightgreen'], alpha=0.7)\n",
    "plt.title('Distribuição das Predições')\n",
    "plt.ylabel('Número de Tweets')\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(i, count + 10, str(count), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos de predições do modelo\n",
    "print(\"5 EXEMPLOS DE PREDIÇÕES DO MODELO:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Função para interpretar sentimento\n",
    "def interpret_sentiment(score):\n",
    "    return \"Positivo\" if score >= 0.5 else \"Negativo\"\n",
    "\n",
    "def interpret_label(label):\n",
    "    return \"Positivo\" if label == 1 else \"Negativo\"\n",
    "\n",
    "# Selecionar 5 exemplos aleatórios do conjunto de teste\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices, 1):\n",
    "    original_text = X_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    pred_proba = y_pred_proba[idx][0]\n",
    "    pred_label = y_pred[idx]\n",
    "    \n",
    "    print(f\"\\nExemplo {i}:\")\n",
    "    print(f\"Tweet original: \\\"{original_text}\\\"\")\n",
    "    print(f\"Rótulo verdadeiro: {interpret_label(true_label)}\")\n",
    "    print(f\"Predição do modelo: {interpret_sentiment(pred_proba)} (confiança: {pred_proba:.3f})\")\n",
    "    \n",
    "    # Indicar se a predição está correta\n",
    "    correct = \"✓\" if pred_label == true_label else \"✗\"\n",
    "    print(f\"Resultado: {correct} {'Correto' if pred_label == true_label else 'Incorreto'}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Estatísticas dos exemplos\n",
    "correct_predictions = sum(y_pred[idx] == y_test[idx] for idx in random_indices)\n",
    "print(f\"\\nDos 5 exemplos mostrados: {correct_predictions}/5 corretos ({correct_predictions/5*100:.1f}%)\")\n",
    "\n",
    "# Função para testar frases personalizadas\n",
    "def predict_sentiment(text, model, tokenizer, max_len=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Prediz o sentimento de um texto personalizado\n",
    "    \"\"\"\n",
    "    # Pré-processar o texto\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    \n",
    "    # Tokenizar e fazer padding\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Fazer predição\n",
    "    prediction = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    \n",
    "    return prediction, interpret_sentiment(prediction)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTE COM FRASES PERSONALIZADAS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Exemplos de teste\n",
    "test_sentences = [\n",
    "    \"I love this movie, it's amazing!\",\n",
    "    \"This is the worst day ever\",\n",
    "    \"I'm feeling great today\",\n",
    "    \"The weather is terrible\",\n",
    "    \"Thank you so much for your help\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    prob, sentiment = predict_sentiment(sentence, model, tokenizer)\n",
    "    print(f\"Frase: \\\"{sentence}\\\"\")\n",
    "    print(f\"Sentimento: {sentiment} (confiança: {prob:.3f})\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d4fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos Resultados e Conclusões\n",
    "print(\"RESUMO DOS RESULTADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"📊 DADOS:\")\n",
    "print(f\"   • Dataset original: 1.6M tweets (usamos amostra de 50k)\")\n",
    "print(f\"   • Conjunto de treino: {len(X_train):,} exemplos\")\n",
    "print(f\"   • Conjunto de teste: {len(X_test):,} exemplos\")\n",
    "print(f\"   • Distribuição balanceada entre sentimentos positivos e negativos\")\n",
    "\n",
    "print(f\"\\n🔧 PRÉ-PROCESSAMENTO:\")\n",
    "print(f\"   • Remoção de URLs, menções (@usuario), hashtags\")\n",
    "print(f\"   • Conversão para minúsculas\")\n",
    "print(f\"   • Remoção de caracteres especiais e números\")\n",
    "print(f\"   • Tokenização com vocabulário de {min(vocab_size, MAX_VOCAB_SIZE):,} palavras\")\n",
    "print(f\"   • Padding para sequências de tamanho {MAX_SEQUENCE_LENGTH}\")\n",
    "\n",
    "print(f\"\\n🧠 ARQUITETURA DO MODELO:\")\n",
    "print(f\"   • Embedding Layer: {EMBEDDING_DIM} dimensões\")\n",
    "print(f\"   • Bidirectional LSTM: {LSTM_UNITS} unidades\")\n",
    "print(f\"   • Dropout: {DROPOUT_RATE} para regularização\")\n",
    "print(f\"   • Dense Layer: 32 neurônios + ReLU\")\n",
    "print(f\"   • Output Layer: 1 neurônio + Sigmoid\")\n",
    "print(f\"   • Total de parâmetros: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\n⚙️ PARÂMETROS DE TREINAMENTO:\")\n",
    "print(f\"   • Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"   • Loss Function: Binary Crossentropy\")\n",
    "print(f\"   • Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   • Épocas máximas: {EPOCHS}\")\n",
    "print(f\"   • Early Stopping: paciência de 5 épocas\")\n",
    "print(f\"   • Learning Rate Reduction: fator 0.5, paciência 3 épocas\")\n",
    "\n",
    "print(f\"\\n📈 RESULTADOS PRINCIPAIS:\")\n",
    "print(f\"   • Accuracy no teste: {test_accuracy:.1%}\")\n",
    "print(f\"   • Precisão: {precision:.3f}\")\n",
    "print(f\"   • Recall: {recall:.3f}\")\n",
    "print(f\"   • F1-Score: {f1:.3f}\")\n",
    "print(f\"   • Épocas executadas: {len(history.history['loss'])}\")\n",
    "\n",
    "print(f\"\\n✅ CONCLUSÕES:\")\n",
    "print(f\"   • O modelo LSTM bidirecional mostrou boa capacidade de generalização\")\n",
    "print(f\"   • A arquitetura simples foi eficaz para o problema de análise de sentimentos\")\n",
    "print(f\"   • O pré-processamento foi fundamental para melhorar a qualidade dos dados\")\n",
    "print(f\"   • Os callbacks evitaram overfitting e otimizaram o treinamento\")\n",
    "print(f\"   • O modelo pode ser usado para classificar sentimentos em novos tweets\")\n",
    "\n",
    "print(f\"\\n🔄 POSSÍVEIS MELHORIAS:\")\n",
    "print(f\"   • Usar embeddings pré-treinados (GloVe, Word2Vec)\")\n",
    "print(f\"   • Experimentar arquiteturas mais complexas (GRU, Transformer)\")\n",
    "print(f\"   • Aumentar o tamanho do dataset de treinamento\")\n",
    "print(f\"   • Aplicar técnicas de data augmentation\")\n",
    "print(f\"   • Implementar ensemble de modelos\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
