---
title: Predi√ß√£o de Sobreviv√™ncia no Titanic
sub_title: Classifica√ß√£o Bin√°ria com MLP
author: Kheven
date: 2025
options:
  end_slide_shorthand: true
theme:
  name: catppuccin-latte
---

# Predi√ß√£o de Sobreviv√™ncia no Titanic

## Classifica√ß√£o Bin√°ria com MLP

---

## Objetivo

**Prever a sobreviv√™ncia de passageiros do Titanic usando redes neurais**

- **Problema**: Classifica√ß√£o bin√°ria (sobreviveu/n√£o sobreviveu)
- **Dataset**: Dados hist√≥ricos de passageiros
- **Arquitetura**: MLP com regulariza√ß√£o
- **Framework**: Keras

---

## O Desastre do Titanic

**Contexto hist√≥rico:**

### O que aconteceu:

- **14 de abril de 1912**: Naufr√°gio do RMS Titanic
- **2.224 pessoas** a bordo
- **1.514 mortes** - uma das maiores trag√©dias mar√≠timas
- **"Mulheres e crian√ßas primeiro"** - protocolo de evacua√ß√£o

### Por que √© importante para ML:

- **Dados reais** com impacto humano
- **Padr√µes sociais** claros nos dados
- **Desbalanceamento** representa situa√ß√µes reais

---

## Dataset e An√°lise Explorat√≥ria

**Caracter√≠sticas dos dados:**

- **891 passageiros** no conjunto de treino
- **418 passageiros** no conjunto de teste
- **Taxa de sobreviv√™ncia**: 38.4% (desbalanceado)
- **12 features** originais + engenharia de caracter√≠sticas

```python
# Carregamento dos dados
train_df = pd.read_csv('DATA/train.csv')
test_df = pd.read_csv('DATA/test.csv')

print(f"Taxa de Sobreviv√™ncia: {train_df['Survived'].mean():.2%}")
```

### Principais vari√°veis:

- **Pclass**: Classe do bilhete (1¬™, 2¬™, 3¬™ classe)
- **Sex**: Sexo do passageiro
- **Age**: Idade
- **Fare**: Tarifa paga
- **Embarked**: Porto de embarque

---

## An√°lise de Valores Ausentes

**Tratamento de dados faltantes:**

| Vari√°vel     | Valores Ausentes | Estrat√©gia                 |
| ------------ | ---------------- | -------------------------- |
| **Age**      | 177 (19.9%)      | Preenchimento com mediana  |
| **Embarked** | 2 (0.2%)         | Preenchimento com moda     |
| **Cabin**    | 687 (77.1%)      | Removida (muitos ausentes) |
| **Fare**     | 1 (teste)        | Preenchimento com mediana  |

```python
# Tratamento de valores ausentes
df['Age'].fillna(df['Age'].median(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
```

**Estrat√©gia conservadora**: Evitar introduzir vi√©s com imputa√ß√µes complexas.

---

## Visualiza√ß√µes Explorat√≥rias

![](output.png)

- _Distribui√ß√£o de sobreviv√™ncia (pizza)_
- _Sobreviv√™ncia por sexo (barras)_
- _Sobreviv√™ncia por classe (barras)_
- _Distribui√ß√£o de idade (histograma)_
- _Sobreviv√™ncia por porto de embarque_
- _Distribui√ß√£o de tarifa_

**Insights visuais:**

- **Mulheres** tiveram taxa de sobreviv√™ncia muito maior
- **1¬™ classe** sobreviveu mais que 2¬™ e 3¬™ classes
- **Crian√ßas** tiveram maior chance de sobreviv√™ncia
- **Porto de embarque** influenciou na sobreviv√™ncia

---

## An√°lise de Correla√ß√£o

![](output2.png)

**Correla√ß√µes principais com sobreviv√™ncia:**

| Vari√°vel        | Correla√ß√£o | Interpreta√ß√£o                         |
| --------------- | ---------- | ------------------------------------- |
| **Sex_encoded** | -0.543     | Ser homem diminui chances             |
| **Pclass**      | -0.338     | Classes mais altas sobrevivem mais    |
| **Fare**        | +0.257     | Tarifas maiores = maior sobreviv√™ncia |
| **Age**         | -0.077     | Idade mais avan√ßada = menor chance    |

```python
# An√°lise de correla√ß√£o
correlation_df = temp_df[numeric_cols].corr()
target_correlation = correlation_df['Survived'].sort_values(key=abs, ascending=False)
```

---

## Feature Engineering

**Cria√ß√£o de novas vari√°veis:**

### 1. Tamanho da Fam√≠lia:

```python
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

# Categoriza√ß√£o
df['FamilySize_Category'] = 'Medium'
df.loc[df['FamilySize'] == 1, 'FamilySize_Category'] = 'Single'
df.loc[df['FamilySize'] >= 5, 'FamilySize_Category'] = 'Large'
```

### 2. Grupos de Idade:

```python
df['AgeGroup'] = 'Adult'
df.loc[df['Age'] <= 16, 'AgeGroup'] = 'Child'
df.loc[df['Age'] >= 60, 'AgeGroup'] = 'Elderly'
```

### 3. Faixas de Tarifa:

```python
df['FareGroup'] = pd.qcut(df['Fare'], 4, labels=['Low', 'Medium', 'High', 'VeryHigh'])
```

---

## Codifica√ß√£o de Vari√°veis

**Transforma√ß√£o categ√≥rica ‚Üí num√©rica:**

```python
def encode_categorical_variables(train_df, test_df):
    categorical_cols = ['Sex', 'Embarked', 'FamilySize_Category',
                       'AgeGroup', 'FareGroup']

    # One-Hot Encoding
    train_encoded = pd.get_dummies(train_df, columns=categorical_cols)
    test_encoded = pd.get_dummies(test_df, columns=categorical_cols)

    # Garantir mesmas colunas em treino e teste
    return train_encoded, test_encoded
```

### Resultado:

- **10 features** finais ap√≥s codifica√ß√£o
- **Consist√™ncia** entre treino e teste
- **Escalabilidade** para novos dados

---

## Normaliza√ß√£o dos Dados

**Padroniza√ß√£o para redes neurais:**

```python
# Separa√ß√£o e normaliza√ß√£o
X = train_encoded.drop('Survived', axis=1)
y = train_encoded['Survived']

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
```

**Por que normalizar?**

- **Diferentes escalas**: Idade (0-80) vs IsAlone (0-1)
- **Converg√™ncia mais r√°pida** do gradiente
- **Estabilidade num√©rica** da rede neural

---

## Arquitetura do Modelo MLP

**Rede neural com regulariza√ß√£o:**

```python
model = Sequential([
    Input(shape=(10,)),                    # 10 features de entrada
    Dense(64, activation='relu'),          # Camada oculta 1
    Dropout(0.3),                         # Regulariza√ß√£o
    Dense(32, activation='relu'),          # Camada oculta 2
    Dense(1, activation='sigmoid')         # Sa√≠da bin√°ria
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

### Especifica√ß√µes:

- **2.817 par√¢metros** trein√°veis
- **ReLU** para camadas ocultas
- **Sigmoid** para probabilidade de sobreviv√™ncia
- **Dropout** para prevenir overfitting

---

## Configura√ß√£o do Treinamento

**Callbacks inteligentes:**

```python
# Callbacks para otimiza√ß√£o autom√°tica
early_stopping = EarlyStopping(
    monitor='val_loss', patience=20, restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss', factor=0.5, patience=10, min_lr=0.0001
)

# Treinamento
history = model.fit(
    X_train_scaled, y_train,
    epochs=200, batch_size=32,
    validation_data=(X_val_scaled, y_val),
    callbacks=[early_stopping, reduce_lr]
)
```

### Benef√≠cios:

- **Early stopping**: Evita overfitting
- **Learning rate reduction**: Melhora converg√™ncia
- **Restaura√ß√£o de pesos**: Usa melhor modelo

---

## Curvas de Treinamento

![image:width:80%](output3.png)

**An√°lise do treinamento:**

- **Converg√™ncia est√°vel** sem oscila√ß√µes bruscas
- **Sem overfitting significativo** (gap treino-valida√ß√£o pequeno)
- **30-50 √©pocas** t√≠picas antes do early stopping
- **Learning rate reduction** melhorou converg√™ncia final

**Estat√≠sticas t√≠picas:**

- Loss final: ~0.44
- Accuracy final: ~82%
- Melhor √©poca: 31
- Early stopping ativado

---

## Resultados Principais

**M√©tricas de performance:**

| M√©trica      | Valor | Interpreta√ß√£o                  |
| ------------ | ----- | ------------------------------ |
| **Accuracy** | 82.1% | Boa classifica√ß√£o geral        |
| **Precis√£o** | 0.82  | Poucos falsos positivos        |
| **Recall**   | 0.78  | Detecta bem sobreviventes      |
| **F1-Score** | 0.80  | Balanceamento precision/recall |

```python
# Avalia√ß√£o final
test_accuracy = accuracy_score(y_val, y_val_pred)
precision = precision_score(y_val, y_val_pred)
recall = recall_score(y_val, y_val_pred)
f1 = f1_score(y_val, y_val_pred)
```

---

## Matriz de Confus√£o

![image:width:80%](output4.png)

**An√°lise dos erros:**

- **Verdadeiros Negativos**: Modelo acerta n√£o-sobreviventes
- **Falsos Positivos**: Prev√™ sobreviv√™ncia incorretamente
- **Falsos Negativos**: N√£o detecta sobreviventes
- **Verdadeiros Positivos**: Acerta sobreviventes

### M√©tricas detalhadas:

- **Sensibilidade**: Capacidade de detectar sobreviventes
- **Especificidade**: Capacidade de detectar n√£o-sobreviventes
- **Precis√£o por classe**: Confiabilidade das predi√ß√µes

---

## An√°lise de Probabilidades

![image:width:80%](output5.png)

üìä **Distribui√ß√£o das predi√ß√µes:**

- **Distribui√ß√£o bimodal**: Maioria pr√≥xima a 0 ou 1
- **Confian√ßa alta**: Modelo √© "decidido" na maioria dos casos
- **Threshold 0.5**: Ponto de corte bem definido

```python
# An√°lise das probabilidades
y_pred_proba = model.predict(X_val_scaled)
print(f"Probabilidade m√©dia: {y_pred_proba.mean():.3f}")
print(f"Desvio padr√£o: {y_pred_proba.std():.3f}")
```

---

## Import√¢ncia das Features

![image:width:70%](output6.png)

**Features mais determinantes:**

| Rank | Feature        | Import√¢ncia | Interpreta√ß√£o                          |
| ---- | -------------- | ----------- | -------------------------------------- |
| 1    | **Pclass**     | 0.122       | Classe social √© crucial                |
| 2    | **Sex_female** | 0.102       | Sexo feminino aumenta muito as chances |
| 3    | **SibSp**      | 0.040       | N√∫mero de irm√£os/c√¥njuges              |
| 4    | **Age**        | 0.039       | Idade influencia sobreviv√™ncia         |
| 5    | **Sex_male**   | 0.037       | Ser homem diminui as chances           |

```python
# C√°lculo da import√¢ncia
perm_importance = permutation_importance(
    model, X_val_scaled, y_val, n_repeats=5, random_state=42
)
```

---

## Predi√ß√µes no Conjunto de Teste

**Resultados finais:**

```python
# Predi√ß√µes finais
test_predictions = (model.predict(X_test_scaled) > 0.5).astype(int)
survival_rate_predicted = sum(test_predictions) / len(test_predictions)

print(f"Total de passageiros: {len(test_predictions)}")
print(f"Sobreviventes preditos: {sum(test_predictions)}")
print(f"Taxa de sobreviv√™ncia predita: {survival_rate_predicted:.1%}")
```

### Compara√ß√£o hist√≥rica:

- **Taxa original**: 38.4%
- **Taxa predita**: 39.7%
- **Alinhamento**: Excelente (diferen√ßa de apenas 1.3%)

---

## Principais Aprendizados

**Insights t√©cnicos:**

### Sobre Regulariza√ß√£o:

- **Dropout** foi essencial para evitar overfitting
- **Early stopping** otimizou automaticamente o treinamento
- **Learning rate scheduling** melhorou converg√™ncia

### Sobre os Dados:

- **Pclass** foi o fator mais determinante
- **Sexo** teve impacto forte na sobreviv√™ncia
- **SibSp** (irm√£os/c√¥njuges) tamb√©m influenciou significativamente
- **Protocolo "mulheres e crian√ßas primeiro"** √© vis√≠vel nos dados

---

## Limita√ß√µes e Desafios

**Pontos de aten√ß√£o:**

### Limita√ß√µes dos dados:

- **Amostra pequena**: 891 exemplos para treino
- **Valores ausentes**: 20% da idade estava faltando

### Desafios do modelo:

- **Desbalanceamento**: 38% vs 62% de sobreviventes
- **Interpretabilidade**: MLP √© menos interpret√°vel que √°rvores
- **Overfitting**: Risco com poucos dados

### Considera√ß√µes √©ticas:

- **Vi√©s hist√≥rico**: Modelo reflete desigualdades de 1912

---

# Obrigado!

## **Pr√≥ximo projeto:** CNN para Classifica√ß√£o CIFAR-10
