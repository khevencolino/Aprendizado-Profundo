---
title: An√°lise de Sentimentos com GRU
sub_title: Processamento de Linguagem Natural para Tweets usando Embeddings Pr√©-treinados
author: Kheven
date: 2025
options:
  end_slide_shorthand: true
---

# An√°lise de Sentimentos com GRU

## Processamento de Linguagem Natural para Tweets usando Embeddings Pr√©-treinados

---

## Objetivo

**Classificar sentimentos em tweets usando GRU Bidirecional**

- **Tarefa**: An√°lise de sentimentos bin√°ria (positivo/negativo)
- **Dataset**: 1.6M tweets do Twitter (amostra de 100k)
- **Arquitetura**: GRU Bidirecional com Embeddings GloVe
- **Framework**: Keras/TensorFlow
- **Estrat√©gia**: Fine-tuning em duas fases

---

## Dataset de Tweets

**Caracter√≠sticas dos dados:**

- **1.6 milh√µes de tweets** originalmente
- **Amostra balanceada**: 50k positivos + 50k negativos
- **Classes**: 0 (negativo) e 4 (positivo) ‚Üí convertido para 0/1
- **Idioma**: Ingl√™s (textos informais de redes sociais)

```python
# Carregamento dos dados
df_full = pd.read_csv('DATA/data.csv', encoding='latin-1', header=None)
df_full.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']

# Amostragem balanceada
df_negative = df_full[df_full['sentiment'] == 0].sample(n=50000)
df_positive = df_full[df_full['sentiment'] == 4].sample(n=50000)
df = pd.concat([df_negative, df_positive])
```

---

## Pr√©-processamento de Texto

üîß **Pipeline de limpeza:**

```python
def preprocess_text(text):
    text = str(text).lower()                    # Min√∫sculas
    text = re.sub(r'http\S+|www.\S+', '', text) # Remover URLs
    text = re.sub(r'@\w+', '', text)            # Remover men√ß√µes
    text = re.sub(r'#', '', text)               # Remover hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)     # S√≥ letras e espa√ßos
    text = re.sub(r'\s+', ' ', text).strip()    # Normalizar espa√ßos
    return text
```

### Exemplos de limpeza:

- **Original**: "@user I love this #movie! üòç http://link.com"
- **Limpo**: "i love this movie"

---

## Tokeniza√ß√£o e Embeddings

**Convers√£o texto ‚Üí vetores num√©ricos:**

```python
# Par√¢metros
MAX_VOCAB_SIZE = 40000      # Vocabul√°rio expandido
MAX_SEQUENCE_LENGTH = 70    # Tamanho fixo das sequ√™ncias
EMBEDDING_DIM = 100         # Dimens√£o dos embeddings GloVe

# Tokeniza√ß√£o
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Convers√£o para sequ√™ncias com padding
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)
```

### Embeddings GloVe:

- **Pr√©-treinados**: 6B tokens, 100 dimens√µes
- **Vantagem**: Conhecimento sem√¢ntico transferido
- **Estrat√©gia**: Fine-tuning em duas fases

---

## Arquitetura do Modelo GRU

**Estrutura da rede neural:**

```python
model = Sequential([
    # Embeddings GloVe pr√©-treinados
    Embedding(
        input_dim=vocab_size,
        output_dim=100,
        weights=[embedding_matrix],
        trainable=False  # Inicialmente congelado
    ),

    # Primeira camada GRU bidirecional
    Bidirectional(GRU(128, dropout=0.3, return_sequences=True)),

    # Segunda camada GRU bidirecional
    Bidirectional(GRU(64, return_sequences=True)),

    # Global Average Pooling
    GlobalAveragePooling1D(),

    # Camadas de classifica√ß√£o
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    # Sa√≠da
    Dense(1, activation='sigmoid')
])
```

---

## Detalhes da Arquitetura

**Componentes principais:**

| Camada                     | Fun√ß√£o                              |
| -------------------------- | ----------------------------------- |
| **Embedding (GloVe)**      | Converte tokens ‚Üí vetores densos    |
| **Bidirectional GRU 1**    | 128 unidades, return_sequences=True |
| **Bidirectional GRU 2**    | 64 unidades, return_sequences=True  |
| **GlobalAveragePooling1D** | Agrega informa√ß√£o da sequ√™ncia      |
| **Dense + BatchNorm**      | 64 neur√¥nios + normaliza√ß√£o         |
| **Dropout**                | Regulariza√ß√£o (30%)                 |
| **Dense (1)**              | Classifica√ß√£o final                 |

### Vantagens da Arquitetura:

- **Duas camadas GRU**: Maior capacidade de abstra√ß√£o
- **GlobalAveragePooling**: Melhor que concatena√ß√£o simples
- **BatchNormalization**: Estabiliza treinamento
- **L2 Regularization**: Previne overfitting

---

## Estrat√©gia de Fine-tuning

**Treinamento em duas fases:**

### Fase 1: Embeddings Congelados (10 √©pocas)

```python
# Embeddings congelados
model.layers[0].trainable = False
model.compile(optimizer=Adam(lr=0.001))

history_phase1 = model.fit(
    X_train_pad, y_train,
    epochs=10, validation_split=0.2,
    callbacks=[EarlyStopping, ReduceLROnPlateau]
)
```

### Fase 2: Fine-tuning (5 √©pocas)

```python
# Descongelar embeddings
model.layers[0].trainable = True
model.compile(optimizer=Adam(lr=0.0001))  # LR reduzido

history_phase2 = model.fit(...)
```

**Por que duas fases?**

- Permite que o modelo aprenda com embeddings est√°veis primeiro
- Fine-tuning refina os embeddings para o dom√≠nio espec√≠fico

---

## Curvas de Treinamento

**An√°lise do treinamento em duas fases:**

![image:width:80%](output.png)

**Observa√ß√µes:**

- **Fase 1**: Converg√™ncia r√°pida com embeddings congelados
- **Fase 2**: Fine-tuning melhora performance gradualmente
- **Early Stopping**: Evita overfitting automaticamente
- **Learning Rate Reduction**: Otimiza√ß√£o refinada

### Estat√≠sticas Finais:

- **√âpocas totais**: ~15 √©pocas
- **Melhor accuracy de valida√ß√£o**: Alcan√ßada na fase 2
- **Estrat√©gia eficaz**: Fine-tuning melhorou os resultados

---

## Resultados Principais

**M√©tricas de performance:**

| M√©trica      | Valor | Interpreta√ß√£o                 |
| ------------ | ----- | ----------------------------- |
| **Accuracy** | ~80%  | Boa classifica√ß√£o geral       |
| **Precis√£o** | ~0.78 | Controla falsos positivos     |
| **Recall**   | ~0.71 | Detecta sentimentos positivos |
| **F1-Score** | ~0.75 | Balanceamento adequado        |

```python
# Avalia√ß√£o no teste
y_pred_proba = model.predict(X_test_pad)
y_pred = (y_pred_proba > 0.5).astype(int)
test_accuracy = accuracy_score(y_test, y_pred)

# Relat√≥rio completo
classification_report(y_test, y_pred,
                     target_names=['Negativo', 'Positivo'])
```

### Vantagens dos Embeddings Pr√©-treinados:

- **Converg√™ncia mais r√°pida** que treinar do zero
- **Melhor generaliza√ß√£o** com vocabul√°rio limitado
- **Conhecimento sem√¢ntico** transferido

---

## Matriz de Confus√£o

![image:width:80%](output2.png)

**An√°lise dos resultados:**

- **Distribui√ß√£o balanceada**: Modelo n√£o favorece uma classe
- **Principais erros**: Textos neutros/amb√≠guos
- **Desafios identificados**:
  - Ironia e sarcasmo
  - Linguagem informal de redes sociais
  - Contexto impl√≠cito

---

## Distribui√ß√£o das Predi√ß√µes

![image:width:80%](output3.png)

**An√°lise das probabilidades:**

- **Bimodal**: Maioria das predi√ß√µes pr√≥ximas a 0 ou 1
- **Confian√ßa alta**: Modelo √© "decidido" na maioria dos casos
- **Threshold 0.5**: Divis√£o clara entre classes

---

## Exemplos de Predi√ß√µes

**Teste com frases personalizadas:**

| Frase                              | Sentimento | Confian√ßa |
| ---------------------------------- | ---------- | --------- |
| "I love this movie, it's amazing!" | Positivo   | 0.92      |
| "This is the worst day ever"       | Negativo   | 0.89      |
| "I'm feeling great today"          | Positivo   | 0.87      |
| "The weather is terrible"          | Negativo   | 0.78      |
| "Thank you so much for your help"  | Positivo   | 0.94      |

```python
def predict_sentiment(text, model, tokenizer):
    cleaned_text = preprocess_text(text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
    prediction = model.predict(padded)[0][0]

    return prediction, "Positivo" if prediction >= 0.5 else "Negativo"
```

### Caracter√≠sticas das Predi√ß√µes:

- **Alta confian√ßa** na maioria dos casos
- **Bom desempenho** em textos claros
- **Desafios** com ambiguidade e ironia

## Conclus√µes

**Principais Descobertas:**

### Arquitetura:

- **GRU Bidirecional**: Eficaz para an√°lise de sentimentos
- **Duas camadas**: Melhor abstra√ß√£o sequencial
- **GlobalAveragePooling**: Superior √† concatena√ß√£o simples

### Embeddings Pr√©-treinados:

- **GloVe**: Vantagem significativa sobre embeddings trein√°veis
- **Fine-tuning**: Estrat√©gia em duas fases foi eficaz
- **Transfer Learning**: Conhecimento sem√¢ntico aproveitado

### Resultados:

- **~80% accuracy**: Performance satisfat√≥ria para tweets
- **Converg√™ncia r√°pida**: ~15 √©pocas total
- **Generaliza√ß√£o**: Boa performance em textos novos

---

# Obrigado!

---
