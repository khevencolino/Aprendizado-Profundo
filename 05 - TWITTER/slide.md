---
title: AnÃ¡lise de Sentimentos com LSTM
sub_title: Processamento de Linguagem Natural para Tweets usando Redes Recorrentes
author: Kheven
date: 2025
options:
  end_slide_shorthand: true
theme:
  name: gruvbox-dark
---

# AnÃ¡lise de Sentimentos com LSTM

## Processamento de Linguagem Natural para Tweets usando Redes Recorrentes

---

## Objetivo

ğŸ¯ **Classificar sentimentos em tweets usando Redes Neurais Recorrentes**

- **Tarefa**: AnÃ¡lise de sentimentos binÃ¡ria (positivo/negativo)
- **Dataset**: 1.6M tweets do Twitter (amostra de 50k)
- **Arquitetura**: LSTM Bidirecional com Embeddings
- **Framework**: Keras

---

## Por que LSTM para AnÃ¡lise de Sentimentos?

ğŸ§  **Vantagens das Redes Recorrentes para texto:**

### Problemas com MLPs tradicionais:

- **Ordem das palavras** importa: "nÃ£o gostei" â‰  "gostei muito"
- **DependÃªncias sequenciais**: contexto e significado
- **Tamanho variÃ¡vel** dos textos

### SoluÃ§Ã£o LSTM:

- **MemÃ³ria de longo prazo**: lembra contexto anterior
- **Processamento sequencial**: palavra por palavra
- **Bidirectional**: analisa texto em ambas as direÃ§Ãµes

---

## Dataset de Tweets

ğŸ“Š **CaracterÃ­sticas dos dados:**

- **1.6 milhÃµes de tweets** originalmente
- **Amostra balanceada**: 25k positivos + 25k negativos
- **Classes**: 0 (negativo) e 4 (positivo) â†’ convertido para 0/1
- **Idioma**: InglÃªs (textos informais de redes sociais)

```python
# Carregamento dos dados
df_full = pd.read_csv('DATA/data.csv', encoding='latin-1', header=None)
df_full.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']

# Amostragem balanceada
df_negative = df_full[df_full['sentiment'] == 0].sample(n=25000)
df_positive = df_full[df_full['sentiment'] == 4].sample(n=25000)
df = pd.concat([df_negative, df_positive])
```

---

## PrÃ©-processamento de Texto

ğŸ”§ **Pipeline de limpeza:**

```python
def preprocess_text(text):
    text = str(text).lower()                    # MinÃºsculas
    text = re.sub(r'http\S+|www.\S+', '', text) # Remover URLs
    text = re.sub(r'@\w+', '', text)            # Remover menÃ§Ãµes
    text = re.sub(r'#', '', text)               # Remover hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)     # SÃ³ letras e espaÃ§os
    text = re.sub(r'\s+', ' ', text).strip()    # Normalizar espaÃ§os
    return text
```

### Exemplos de limpeza:

- **Original**: "@user I love this #movie! ğŸ˜ http://link.com"
- **Limpo**: "i love this movie"

---

## TokenizaÃ§Ã£o e SequÃªncias

ğŸ“ **ConversÃ£o texto â†’ nÃºmeros:**

```python
# ParÃ¢metros
MAX_VOCAB_SIZE = 20000      # VocabulÃ¡rio mÃ¡ximo
MAX_SEQUENCE_LENGTH = 70    # Tamanho fixo das sequÃªncias
EMBEDDING_DIM = 100         # DimensÃ£o dos embeddings

# TokenizaÃ§Ã£o
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# ConversÃ£o para sequÃªncias
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)
```

### Processo:

1. **"i love this movie"** â†’ **[15, 243, 89, 156]**
2. **Padding**: **[15, 243, 89, 156, 0, 0, ..., 0]** (atÃ© 70 tokens)

---

## Arquitetura do Modelo LSTM

ğŸ—ï¸ **Estrutura da rede neural:**

```python
model = Sequential([
    # Camada de Embedding
    Embedding(
        input_dim=20000,      # VocabulÃ¡rio
        output_dim=100,       # DimensÃ£o do embedding
        input_length=70       # Tamanho da sequÃªncia
    ),

    # LSTM Bidirecional
    Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.5)),

    # Camadas densas
    Dense(64, activation='relu'),
    Dropout(0.5),

    # SaÃ­da
    Dense(1, activation='sigmoid')  # ClassificaÃ§Ã£o binÃ¡ria
])
```

---

## Detalhes da Arquitetura

ğŸ“ **Componentes principais:**

| Camada                 | ParÃ¢metros | FunÃ§Ã£o                           |
| ---------------------- | ---------- | -------------------------------- |
| **Embedding**          | 2M         | Converte tokens â†’ vetores densos |
| **Bidirectional LSTM** | 165K       | Processa sequÃªncia (â†’ + â†)       |
| **Dense (64)**         | 16K        | ExtraÃ§Ã£o de features             |
| **Dropout**            | -          | RegularizaÃ§Ã£o (50%)              |
| **Dense (1)**          | 65         | ClassificaÃ§Ã£o final              |

**Total**: ~2.2M parÃ¢metros treinÃ¡veis

### Bidirectional LSTM:

- **Forward**: lÃª da esquerda para direita
- **Backward**: lÃª da direita para esquerda
- **Concatena** ambas as representaÃ§Ãµes

---

## ConfiguraÃ§Ã£o do Treinamento

âš™ï¸ **HiperparÃ¢metros e callbacks:**

```python
# CompilaÃ§Ã£o
model.compile(
    optimizer=Adam(learning_rate=0.01),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Callbacks inteligentes
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),
    ModelCheckpoint(save_weights_only=True)
]

# Treinamento
history = model.fit(
    X_train_pad, y_train,
    batch_size=128, epochs=10,
    validation_split=0.2,
    callbacks=callbacks
)
```

---

## Curvas de Treinamento

ğŸ“ˆ **AnÃ¡lise do treinamento:**

- ConvergÃªncia estÃ¡vel sem overfitting
- Early stopping otimizou nÃºmero de Ã©pocas
- Learning rate reduction melhorou convergÃªncia final

**EstatÃ­sticas:**

- Ã‰pocas executadas: 7-10
- Loss final: ~0.35
- Accuracy final: ~85%

---

## Resultados Principais

ğŸ† **MÃ©tricas de performance:**

| MÃ©trica      | Valor  | InterpretaÃ§Ã£o                     |
| ------------ | ------ | --------------------------------- |
| **Accuracy** | 83-85% | Boa classificaÃ§Ã£o geral           |
| **PrecisÃ£o** | 0.84   | Poucos falsos positivos           |
| **Recall**   | 0.85   | Detecta bem sentimentos positivos |
| **F1-Score** | 0.84   | Balanceamento precisÃ£o/recall     |

```python
# AvaliaÃ§Ã£o no teste
y_pred_proba = model.predict(X_test_pad)
y_pred = (y_pred_proba > 0.5).astype(int)
test_accuracy = accuracy_score(y_test, y_pred)
```

---

## Matriz de ConfusÃ£o

![image:width:80%](output2.png)

**AnÃ¡lise dos erros:**

- **Falsos Positivos**: Textos neutros classificados como positivos
- **Falsos Negativos**: Ironia/sarcasmo classificados incorretamente
- **Desafios**: Contexto implÃ­cito, linguagem informal

---

## DistribuiÃ§Ã£o das PrediÃ§Ãµes

![image:width:80%](output3.png)

ğŸ“Š **AnÃ¡lise das probabilidades:**

- **Bimodal**: Maioria das prediÃ§Ãµes prÃ³ximas a 0 ou 1
- **ConfianÃ§a alta**: Modelo Ã© "decidido" na maioria dos casos
- **Threshold 0.5**: DivisÃ£o clara entre classes

---

## Exemplos de PrediÃ§Ãµes

**Teste com frases personalizadas:**

| Frase                              | Sentimento | ConfianÃ§a |
| ---------------------------------- | ---------- | --------- |
| "I love this movie, it's amazing!" | Positivo   | 0.92      |
| "This is the worst day ever"       | Negativo   | 0.89      |
| "I'm feeling great today"          | Positivo   | 0.87      |
| "The weather is terrible"          | Negativo   | 0.78      |
| "Thank you so much for your help"  | Positivo   | 0.94      |

```python
def predict_sentiment(text, model, tokenizer):
    cleaned_text = preprocess_text(text)
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
    prediction = model.predict(padded)[0][0]
    return prediction, "Positivo" if prediction >= 0.5 else "Negativo"
```

---

## Como o Modelo "Entende" Texto

ğŸ” **Processo interno:**

### 1. Embedding Layer:

- **"love"** â†’ **[0.12, -0.45, 0.78, ...]** (100 dimensÃµes)
- Palavras similares tÃªm vetores prÃ³ximos

### 2. LSTM Bidirecional:

- **Forward**: "i" â†’ "love" â†’ "this" â†’ "movie"
- **Backward**: "movie" â†’ "this" â†’ "love" â†’ "i"
- Combina contexto de ambas as direÃ§Ãµes

### 3. Dense Layers:

- Mapeia representaÃ§Ã£o LSTM â†’ probabilidade final
- Dropout previne decorar exemplos especÃ­ficos

---

## Principais Aprendizados

ğŸ“ **Insights tÃ©cnicos:**

### Sobre LSTMs:

- **MemÃ³ria seletiva**: Esquece informaÃ§Ã£o irrelevante
- **Bidirectional**: Crucial para entender contexto completo
- **Dropout**: Essencial para generalizaÃ§Ã£o

### Sobre Dados:

- **PrÃ©-processamento** Ã© fundamental para qualidade
- **VocabulÃ¡rio limitado** funciona bem
- **SequÃªncias padronizadas** facilitam processamento

### Sobre Treinamento:

- **Early stopping** previne overfitting
- **Learning rate scheduling** melhora convergÃªncia
- **Validation split** monitora generalizaÃ§Ã£o

---

## LimitaÃ§Ãµes e Desafios

âš ï¸ **Pontos de atenÃ§Ã£o:**

### LimitaÃ§Ãµes do modelo:

- **Ironia e sarcasmo**: DifÃ­cil de detectar
- **Contexto cultural**: ExpressÃµes idiomÃ¡ticas
- **Textos curtos**: Pouco contexto disponÃ­vel
- **Emojis**: Removidos no prÃ©-processamento

---

## ConclusÃµes

âœ… **Objetivos alcanÃ§ados:**

- ğŸ¯ **ClassificaÃ§Ã£o eficaz** de sentimentos em tweets
- ğŸ“ˆ **Accuracy de 83-85%** competitiva para a tarefa
- ğŸ§  **LSTM bidirecional** capturou dependÃªncias sequenciais

---

# Obrigado!

## Perguntas?

---
